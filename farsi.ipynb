{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "import nltk\n",
    "import pandas\n",
    "import re \n",
    "import pickle\n",
    "import arabic_reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2824 entries, 0 to 2823\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     2824 non-null   object\n",
      " 1   date      2824 non-null   object\n",
      " 2   content   2441 non-null   object\n",
      " 3   category  2824 non-null   object\n",
      " 4   author    2824 non-null   object\n",
      " 5   comments  2824 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 132.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_excel('final_books.xlsx')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xa0\\xa0مترجم در پیشگفتار این گونه بیان می کند که کتاب “سلسله های اسلامی” که از نظر خوانندگان می گذرد، می تواند یک کتاب مراجعه کوچک در زمینه تاریخ اسلام به شمار آید، و مراد من از کتاب مراجعه، آن چنان کتابی است که فرنگی ها بدان کتاب رفرنس می گویندو تفاوت چنین کتابی با کتابهای معمولی که درباره تاریخ کشورهای اسلامی نوشته شده است در این است که این کتاب، و اصولا همه کتابهای مراجعه، برای آن نیست که خواننده از آغاز تا انجام آن یکباره مطالعه کند، بلکه برای آن است که برای رفع احتیاجات فوری خود بدان مراجعه کند، و جواب پرسش یا مشکل خود را در آن بیابد، و به این ترتیب، با صرف مقدار ناچیزی از وقت به آنچه می خواهد برسد.,مترجم در پیشگفتار این گونه بیان می کند که کتاب “سلسله های اسلامی” که از نظر خوانندگان می گذرد، می تواند یک کتاب مراجعه کوچک در زمینه تاریخ اسلام به شمار آید، و مراد من از کتاب مراجعه، آن چنان کتابی است که فرنگی ها بدان کتاب رفرنس می گویند و تفاوت چنین کتابی با کتابهای معمولی که درباره تاریخ کشورهای اسلامی نوشته شده است در این است که این کتاب، و اصولا همه کتابهای مراجعه، برای آن نیست که خواننده از آغاز تا انجام آن یکباره مطالعه کند، بلکه برای آن است که برای رفع احتیاجات فوری خود بدان مراجعه کند، و جواب پرسش یا مشکل خود را در آن بیابد، و به این ترتیب، با صرف مقدار ناچیزی از وقت به آنچه می خواهد برسد.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[502]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~،'''\n",
    "    for char in text:\n",
    "        if char in punctuations:\n",
    "            text = text.replace(char, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "stop_words = hazm.stopwords_list()\n",
    "\n",
    "tokens = []\n",
    "lemmatized_tokens = []\n",
    "for index, row in df.iterrows():\n",
    "    content = row['content'] \n",
    "    norm_content = normalizer.normalize(str(content))\n",
    "    norm_content_no_punctuations = remove_punctuations(norm_content)\n",
    "    tokenized_sentences = hazm.sent_tokenize(norm_content_no_punctuations)\n",
    "    content_tokenized_words = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        try:\n",
    "            tokenized_words = hazm.word_tokenize(sentence)\n",
    "            filtered_words = [word for word in tokenized_words if word not in stop_words]\n",
    "            content_tokenized_words.extend(filtered_words)\n",
    "            for word in filtered_words:\n",
    "                lemmatizedWord = lemmatizer.lemmatize(word)\n",
    "                lemmatized_tokens.append((index, lemmatizedWord))\n",
    "        except:\n",
    "            print(\"error\")  \n",
    "    tokens.extend(content_tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index(arr):\n",
    "    word_index = {}\n",
    "    for doc_id, word in arr:\n",
    "        if word in word_index:\n",
    "            word_index[word].append(doc_id)\n",
    "        else:\n",
    "            word_index[word] = [doc_id]\n",
    "    return dict(sorted(word_index.items(), key=lambda x: len(x[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = create_word_index(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کتاب [0, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 10, 11, 13, 13, 13, 13, 13, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 18, 18, 19, 21, 21, 21, 21, 22, 24, 25, 25, 26, 27, 27, 27, 29, 30, 30, 30, 34, 35, 37, 37, 37, 38, 39, 40, 41, 43, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 48, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "first_key, first_value = next(iter(word_index.items()))\n",
    "print(first_key, first_value[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_values(d):\n",
    "    sorted_dict = {}\n",
    "    for k in d:\n",
    "        sorted_dict[k] = sorted(set(d[k]), key=d[k].count, reverse=True)\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کتاب [502, 512, 428, 960, 1583, 103, 112, 188, 544, 8]\n"
     ]
    }
   ],
   "source": [
    "sorted_word_index = sort_dict_values(word_index)\n",
    "first_key, first_value = next(iter(sorted_word_index.items()))\n",
    "print(first_key, first_value[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word_dictionaries object to a file\n",
    "with open('index_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(sorted_word_index, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
